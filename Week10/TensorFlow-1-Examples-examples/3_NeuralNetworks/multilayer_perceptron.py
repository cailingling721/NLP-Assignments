'''
A Multilayer Perceptron implementation example using TensorFlow library.
This example is using the MNIST database of handwritten digits
(http://yann.lecun.com/exdb/mnist/)

Author: Aymeric Damien
Project: https://github.com/aymericdamien/TensorFlow-Examples/
'''

from __future__ import print_function

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)

import tensorflow as tf

# Parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100
display_step = 1

# Network Parameters
n_hidden_1 = 256 # 1st layer number of features
n_hidden_2 = 256 # 2nd layer number of features
n_input = 784 # MNIST data input (img shape: 28*28)
n_classes = 10 # MNIST total classes (0-9 digits)

# tf Graph input
x = tf.placeholder("float", [None, n_input])
y = tf.placeholder("float", [None, n_classes])


# Create model
def multilayer_perceptron(x, weights, biases):
    
    # Hidden layer with RELU activation
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.relu(layer_1)
    
    # Hidden layer with RELU activation
    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    layer_2 = tf.nn.relu(layer_2)
    
    # Output layer with linear activation
    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']
    return out_layer

# Store layers weight & bias
weights = {
    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
}
biases = {
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'b2': tf.Variable(tf.random_normal([n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

# Construct model
pred = multilayer_perceptron(x, weights, biases)

# Define loss and optimizer
"""
# tf.nn.softmax_cross_entropy_with_logits:é¦–å…ˆpredç»è¿‡softmaxå‡½æ•°å˜ä¸ºç±»åˆ«æ¦‚ç‡ï¼Œç„¶åå†è®¡ç®—äº¤å‰ç†µï¼Œç›¸å½“äºsoftmaxå’Œäº¤å‰ç†µçš„è®¡ç®—åˆäºŒä¸ºä¸€ã€‚
# é¢„æµ‹è¶Šå‡†ç¡®ï¼Œ ï¼ˆ- æ±‚å’Œï¼ˆçœŸå®y * logï¼ˆé¢„æµ‹y ï¼‰ï¼‰ï¼‰ï¼Œå³äº¤å‰ç†µçš„ç»“æœå€¼è¶Šå°ï¼ˆåˆ«å¿˜äº†å‰é¢è¿˜æœ‰è´Ÿå·ï¼‰
# è¿™ä¸ªå‡½æ•°çš„è¿”å›å€¼å¹¶ä¸æ˜¯ä¸€ä¸ªæ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå¦‚æœè¦æ±‚äº¤å‰ç†µï¼Œæˆ‘ä»¬è¦å†åšä¸€æ­¥tf.reduce_sumæ“ä½œ, å°±æ˜¯å¯¹å‘é‡é‡Œé¢æ‰€æœ‰å…ƒç´ æ±‚å’Œã€‚
# å¦‚æœæ±‚lossï¼Œåˆ™è¦åšä¸€æ­¥tf.reduce_meanæ“ä½œï¼Œå¯¹å‘é‡æ±‚å‡å€¼ï¼

# é€»è¾‘å›å½’
# Construct model
pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax

# Minimize error using cross entropy
# äº¤å‰ç†µçš„è®¡ç®—ï¼š- æ±‚å’Œï¼ˆçœŸå®y * log(é¢„æµ‹y)ï¼‰
cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))
"""

"""
sigmoid å’Œ softmaxçš„å¯¹æ¯”ï¼šrefer to https://blog.nex3z.com/2017/05/02/sigmoid-å‡½æ•°å’Œ-softmax-å‡½æ•°çš„åŒºåˆ«å’Œå…³ç³»/
    Sigmoid å‡½æ•°å½¢å¼ä¸ºï¼š
        ğ‘†(ğ‘¥)=1/ï¼ˆ1+ğ‘’âˆ’ğ‘¥ï¼‰
        Sigmoid æ˜¯ä¸€ä¸ªå¯å¾®çš„æœ‰ç•Œå‡½æ•°ï¼Œåœ¨å„ç‚¹å‡æœ‰éè´Ÿçš„å¯¼æ•°ã€‚å½“ ğ‘¥â†’âˆ æ—¶ï¼Œğ‘†(ğ‘¥)â†’1ï¼›å½“ ğ‘¥â†’âˆ’âˆ æ—¶ï¼Œğ‘†(ğ‘¥)â†’0ã€‚
       å¸¸ç”¨äºäºŒå…ƒåˆ†ç±»ï¼ˆBinary Classificationï¼‰é—®é¢˜ï¼Œä»¥åŠç¥ç»ç½‘ç»œçš„æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰ï¼ˆæŠŠçº¿æ€§çš„è¾“å…¥è½¬æ¢ä¸ºéçº¿æ€§çš„è¾“å‡ºï¼‰ã€‚  
    Softmax å‡½æ•°å½¢å¼ä¸ºï¼š
        ğ‘†(ğ‘¥ğ‘—)=ğ‘’ğ‘¥ğ‘—âˆ‘ğ¾ğ‘˜=1ğ‘’ğ‘¥ğ‘˜,ğ‘—=1,2,â€¦,ğ¾
        å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º K çš„ä»»æ„å®æ•°çŸ¢é‡ï¼ŒSoftmax å¯ä»¥æŠŠå®ƒå‹ç¼©ä¸ºä¸€ä¸ªé•¿åº¦ä¸º K çš„ã€å–å€¼åœ¨ (0, 1) åŒºé—´çš„å®æ•°çŸ¢é‡ï¼Œä¸”çŸ¢é‡ä¸­å„å…ƒç´ ä¹‹å’Œä¸º 1ã€‚
        å®ƒåœ¨å¤šå…ƒåˆ†ç±»ï¼ˆMulticlass Classificationï¼‰å’Œç¥ç»ç½‘ç»œä¸­ä¹Ÿæœ‰å¾ˆå¤šåº”ç”¨ã€‚Softmax ä¸åŒäºæ™®é€šçš„ max å‡½æ•°ï¼šmax å‡½æ•°åªè¾“å‡ºæœ€å¤§çš„é‚£ä¸ªå€¼ï¼Œ
        è€Œ Softmax åˆ™ç¡®ä¿è¾ƒå°çš„å€¼ä¹Ÿæœ‰è¾ƒå°çš„æ¦‚ç‡ï¼Œä¸ä¼šè¢«ç›´æ¥èˆå¼ƒæ‰ï¼Œæ˜¯ä¸€ä¸ªæ¯”è¾ƒâ€œSoftâ€çš„â€œmaxâ€ã€‚
"""
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(mnist.train.num_examples/batch_size)
        
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch
        
        # Display logs per epoch step
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))
    
    print("Optimization Finished!")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    print("Accuracy:", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))
