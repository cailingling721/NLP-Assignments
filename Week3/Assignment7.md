## 190903 Assignment7

### æ•°æ®é¢„å¤„ç†

è¿™é‡Œçš„æ•°æ®é¢„å¤„ç†ä¸»è¦åŒ…æ‹¬å»é™¤æ–‡ä»¶ä¸­çš„ç¼ºå¤±å€¼ã€‚åŸæœ¬æ–°é—»è¯­æ–™ä¸­å«æœ‰89611ä¸ªæ–°é—»ï¼Œä½†æ˜¯å…¶ä¸­åªæœ‰87052ä¸ªæ–°é—»å†…å®¹('content')ä¸ä¸ºç©ºã€‚å¦å¤–ï¼Œè¿˜éœ€è¦ç»™æ•°æ®æ·»åŠ æ ‡ç­¾ã€‚æ–°åç¤¾çš„æ–°é—»æ ‡è®°ä¸º1ï¼Œå…¶ä½™æ¥æºçš„æ–°é—»æ ‡è®°ä¸º0ã€‚

è¿™é‡Œå¯ä»¥è¯»å–å·²ç»åˆ†å¥½è¯çš„æ–‡ä»¶ï¼ŒèŠ‚çœæ—¶é—´ã€‚ä¹Ÿå¯ä»¥å¤„ç†çš„æ—¶å€™å†ä¸€å¥å¥åˆ†è¯ã€‚

```python
import pandas as pd
csv_path = "/data/tusers/lixiangr/caill/NLP/data/sqlResult_1558435.csv"
news = pd.read_csv(csv_path, encoding = 'gb18030')
news_nona = news.dropna(subset = ['source', 'content'])

# æ ‡ç­¾
source = news_nona['source'].tolist()
y = [1 if source[i] == 'æ–°åç¤¾' else 0 for i in range(len(news_nona))]
y_ = pd.Series([y], index=['y'])

# è¯»å–å·²ç»åˆ†å¥½è¯çš„æ–‡ä»¶ï¼Œå¹¶ä¸”æ’å…¥yä¸€åˆ—
corpus = pd.read_csv("/data/tusers/lixiangr/caill/NLP/data/news-sentences-cut.txt", header = None, sep = "\t")
corpus.columns = ['content']
corpus.insert(0, 'y', y)
news_content = corpus['content'].tolist()
```

```python

y.count(0) # 8391
y.count(1) # 78661

# pos/neg = 9.374448814205696
```


### æ„å»ºæ–°é—»æ–‡æœ¬çš„TF-IDFå‘é‡

è¿™é‡Œéœ€è¦ä½¿ç”¨æ‰€æœ‰çš„æ–°é—»é¢„æ–™æ„å»ºTF-IDFå‘é‡ã€‚ç„¶åå†å»åˆ’åˆ†éªŒè¯é›†ã€æµ‹è¯•é›†å’Œè®­ç»ƒé›†ã€‚ç”Ÿæˆçš„Xå³TF-IDFå‘é‡ï¼ŒXçš„æ¯ä¸€è¡Œä»£è¡¨ä¸€æ¡æ–°é—»ï¼Œæ¯ä¸€åˆ—ä»£è¡¨æ¯ä¸€ä¸ªè¯ã€‚å¯ä»¥ä½¿ç”¨max_featuresæ§åˆ¶æ‰€ä½¿ç”¨çš„è¯çš„æ•°ç›®ã€‚å¯¹æ‰€æœ‰å…³é”®è¯çš„term frequencyè¿›è¡Œé™åºæ’åºï¼Œåªå–å‰max_featuresä¸ªä½œä¸ºå…³é”®è¯é›†ã€‚

```python

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features = 500)
X = vectorizer.fit_transform(news_content)

X.shape
X.toarray()
word = vectorizer.get_feature_names()
```

åˆ’åˆ†éªŒè¯é›†ï¼ˆvalidationï¼‰ã€æµ‹è¯•é›†ï¼ˆtestï¼‰å’Œè®­ç»ƒé›†ï¼ˆtrainingï¼‰ã€‚åé¢æˆ‘ä»¬ä½¿ç”¨trainingæ•°æ®é›†è¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼Œvalidationæ•°æ®è¿›è¡ŒéªŒè¯ã€‚

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=1)

X_train.shape
# (62676, 500)
X_val.shape
# (15670, 500)
X_test.shape
# (8706, 500)
```


### KNN



åœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹æ‹Ÿåˆæ•°æ®åï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œé¢„æµ‹ã€‚æ­¤æ—¶é¢„æµ‹æœ‰ä¸‰ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬predictï¼Œpredict_log_probaå’Œpredict_probaã€‚

* predictæ–¹æ³•å°±æ˜¯æˆ‘ä»¬æœ€å¸¸ç”¨çš„é¢„æµ‹æ–¹æ³•ï¼Œç›´æ¥ç»™å‡ºæµ‹è¯•é›†çš„é¢„æµ‹ç±»åˆ«è¾“å‡ºã€‚

* predict_probaåˆ™ä¸åŒï¼Œå®ƒä¼šç»™å‡ºæµ‹è¯•é›†æ ·æœ¬åœ¨å„ä¸ªç±»åˆ«ä¸Šé¢„æµ‹çš„æ¦‚ç‡ã€‚å®¹æ˜“ç†è§£ï¼Œpredict_probaé¢„æµ‹å‡ºçš„å„ä¸ªç±»åˆ«æ¦‚ç‡é‡Œçš„æœ€å¤§å€¼å¯¹åº”çš„ç±»åˆ«ï¼Œä¹Ÿå°±æ˜¯predictæ–¹æ³•å¾—åˆ°ç±»åˆ«ã€‚

* predict_log_probaå’Œpredict_probaç±»ä¼¼ï¼Œå®ƒä¼šç»™å‡ºæµ‹è¯•é›†æ ·æœ¬åœ¨å„ä¸ªç±»åˆ«ä¸Šé¢„æµ‹çš„æ¦‚ç‡çš„ä¸€ä¸ªå¯¹æ•°è½¬åŒ–ã€‚è½¬åŒ–åpredict_log_probaé¢„æµ‹å‡ºçš„å„ä¸ªç±»åˆ«å¯¹æ•°æ¦‚ç‡é‡Œçš„æœ€å¤§å€¼å¯¹åº”çš„ç±»åˆ«ï¼Œä¹Ÿå°±æ˜¯predictæ–¹æ³•å¾—åˆ°ç±»åˆ«ã€‚

å»ºæ¨¡ä¹‹åï¼Œä½¿ç”¨å‡†ç¡®ç‡ã€æŸ¥å‡†ç‡ã€æŸ¥å…¨ç‡ã€F1 scoreã€PR-AUCä»¥åŠROC-AUCè¿›è¡Œè¯„ä¼°ã€‚


```python
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, auc
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import average_precision_score, precision_recall_curve

neigh = KNeighborsClassifier(n_neighbors=3) # n_neighborsçš„ç¼ºçœå€¼æ˜¯5
neigh.fit(X_train, y_train) 

neigh.score(X_val, y_val)
pred = neigh.predict(X_val)
pred_prob = neigh.predict_proba(X_val)

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9390169284111768

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.977979182893153

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9581021087680355

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.8018761064085546

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
area = auc(recall, precision)
# 0.9778175881204517
```

#### KNNè°ƒå‚


* Kå€¼çš„é€‰æ‹©ä¸æ ·æœ¬åˆ†å¸ƒæœ‰å…³ï¼Œä¸€èˆ¬é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„Kå€¼ï¼Œå¯ä»¥é€šè¿‡äº¤å‰éªŒè¯æ¥é€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒä¼˜çš„Kå€¼ï¼Œé»˜è®¤å€¼æ˜¯5ã€‚å¦‚æœæ•°æ®æ˜¯ä¸‰ç»´ä¸€ä¸‹çš„ï¼Œå¦‚æœæ•°æ®æ˜¯ä¸‰ç»´æˆ–è€…ä¸‰ç»´ä»¥ä¸‹çš„ï¼Œå¯ä»¥é€šè¿‡å¯è§†åŒ–è§‚å¯Ÿæ¥è°ƒå‚ã€‚

* å‚æ•°è¿‘é‚»æƒweightsï¼šä¸»è¦ç”¨äºæ ‡è¯†æ¯ä¸ªæ ·æœ¬çš„è¿‘é‚»æ ·æœ¬çš„æƒé‡ï¼Œå¦‚æœæ˜¯KNNï¼Œå°±æ˜¯Kä¸ªè¿‘é‚»æ ·æœ¬çš„æƒé‡ï¼Œå¦‚æœæ˜¯é™å®šåŠå¾„æœ€è¿‘é‚»ï¼Œå°±æ˜¯åœ¨è·ç¦»åœ¨åŠå¾„ä»¥å†…çš„è¿‘é‚»æ ·æœ¬çš„æƒé‡ã€‚å¯ä»¥é€‰æ‹©"uniform","distance" æˆ–è€…è‡ªå®šä¹‰æƒé‡ã€‚é€‰æ‹©é»˜è®¤çš„"uniform"ï¼Œæ„å‘³ç€æ‰€æœ‰æœ€è¿‘é‚»æ ·æœ¬æƒé‡éƒ½ä¸€æ ·ï¼Œåœ¨åšé¢„æµ‹æ—¶ä¸€è§†åŒä»ã€‚å¦‚æœæ˜¯"distance"ï¼Œåˆ™æƒé‡å’Œè·ç¦»æˆåæ¯”ä¾‹ï¼Œå³è·ç¦»é¢„æµ‹ç›®æ ‡æ›´è¿‘çš„è¿‘é‚»å…·æœ‰æ›´é«˜çš„æƒé‡ï¼Œè¿™æ ·åœ¨é¢„æµ‹ç±»åˆ«æˆ–è€…åšå›å½’æ—¶ï¼Œæ›´è¿‘çš„è¿‘é‚»æ‰€å çš„å½±å“å› å­ä¼šæ›´åŠ å¤§ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è‡ªå®šä¹‰æƒé‡ï¼Œå³è‡ªå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥æ˜¯è·ç¦»å€¼ï¼Œè¾“å‡ºæ˜¯æƒé‡å€¼ã€‚è¿™æ ·æˆ‘ä»¬å¯ä»¥è‡ªå·±æ§åˆ¶ä¸åŒçš„è·ç¦»æ‰€å¯¹åº”çš„æƒé‡ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæ ·æœ¬çš„åˆ†å¸ƒæ˜¯æ¯”è¾ƒæˆç°‡çš„ï¼Œå³å„ç±»æ ·æœ¬éƒ½åœ¨ç›¸å¯¹åˆ†å¼€çš„ç°‡ä¸­æ—¶ï¼Œæˆ‘ä»¬ç”¨é»˜è®¤çš„"uniform"å°±å¯ä»¥äº†ï¼Œå¦‚æœæ ·æœ¬çš„åˆ†å¸ƒæ¯”è¾ƒä¹±ï¼Œè§„å¾‹ä¸å¥½å¯»æ‰¾ï¼Œé€‰æ‹©"distance"æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„é€‰æ‹©ã€‚å¦‚æœç”¨"distance"å‘ç°é¢„æµ‹çš„æ•ˆæœçš„è¿˜æ˜¯ä¸å¥½ï¼Œå¯ä»¥è€ƒè™‘è‡ªå®šä¹‰è·ç¦»æƒé‡æ¥è°ƒä¼˜è¿™ä¸ªå‚æ•°ã€‚

* KNNå’Œé™å®šåŠå¾„æœ€è¿‘é‚»æ³•ä½¿ç”¨çš„ç®—æ³•algorithm ï¼šç®—æ³•ä¸€å…±æœ‰ä¸‰ç§ï¼Œç¬¬ä¸€ç§æ˜¯è›®åŠ›å®ç°ï¼Œç¬¬äºŒç§æ˜¯KDæ ‘å®ç°ï¼Œç¬¬ä¸‰ç§æ˜¯çƒæ ‘å®ç°ã€‚å¯¹äºè¿™ä¸ªå‚æ•°ï¼Œä¸€å…±æœ‰4ç§å¯é€‰è¾“å…¥ï¼Œâ€˜bruteâ€™å¯¹åº”ç¬¬ä¸€ç§è›®åŠ›å®ç°ï¼Œâ€˜kd_treeâ€™å¯¹åº”ç¬¬äºŒç§KDæ ‘å®ç°ï¼Œâ€˜ball_treeâ€™å¯¹åº”ç¬¬ä¸‰ç§çš„çƒæ ‘å®ç°ï¼Œ â€˜autoâ€™åˆ™ä¼šåœ¨ä¸Šé¢ä¸‰ç§ç®—æ³•ä¸­åšæƒè¡¡ï¼Œé€‰æ‹©ä¸€ä¸ªæ‹Ÿåˆæœ€å¥½çš„æœ€ä¼˜ç®—æ³•ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœè¾“å…¥æ ·æœ¬ç‰¹å¾æ˜¯ç¨€ç–çš„æ—¶å€™ï¼Œæ— è®ºæˆ‘ä»¬é€‰æ‹©å“ªç§ç®—æ³•ï¼Œæœ€åscikit-learnéƒ½ä¼šå»ç”¨è›®åŠ›å®ç°â€˜bruteâ€™ã€‚

ä¸ªäººçš„ç»éªŒï¼Œå¦‚æœæ ·æœ¬å°‘ç‰¹å¾ä¹Ÿå°‘ï¼Œä½¿ç”¨é»˜è®¤çš„ â€˜autoâ€™å°±å¤Ÿäº†ã€‚ å¦‚æœæ•°æ®é‡å¾ˆå¤§æˆ–è€…ç‰¹å¾ä¹Ÿå¾ˆå¤šï¼Œç”¨"auto"å»ºæ ‘æ—¶é—´ä¼šå¾ˆé•¿ï¼Œæ•ˆç‡ä¸é«˜ï¼Œå»ºè®®é€‰æ‹©KDæ ‘å®ç°â€˜kd_treeâ€™ï¼Œæ­¤æ—¶å¦‚æœå‘ç°â€˜kd_treeâ€™é€Ÿåº¦æ¯”è¾ƒæ…¢æˆ–è€…å·²ç»çŸ¥é“æ ·æœ¬åˆ†å¸ƒä¸æ˜¯å¾ˆå‡åŒ€æ—¶ï¼Œå¯ä»¥å°è¯•ç”¨â€˜ball_treeâ€™ã€‚è€Œå¦‚æœè¾“å…¥æ ·æœ¬æ˜¯ç¨€ç–çš„ï¼Œæ— è®ºä½ é€‰æ‹©å“ªä¸ªç®—æ³•æœ€åå®é™…è¿è¡Œçš„éƒ½æ˜¯â€˜bruteâ€™ã€‚

* åœæ­¢å»ºå­æ ‘çš„å¶å­èŠ‚ç‚¹é˜ˆå€¼leaf_sizeï¼šè¿™ä¸ªå€¼æ§åˆ¶äº†ä½¿ç”¨KDæ ‘æˆ–è€…çƒæ ‘æ—¶ï¼Œ åœæ­¢å»ºå­æ ‘çš„å¶å­èŠ‚ç‚¹æ•°é‡çš„é˜ˆå€¼ã€‚è¿™ä¸ªå€¼è¶Šå°ï¼Œåˆ™ç”Ÿæˆçš„KDæ ‘æˆ–è€…çƒæ ‘å°±è¶Šå¤§ï¼Œå±‚æ•°è¶Šæ·±ï¼Œå»ºæ ‘æ—¶é—´è¶Šé•¿ï¼Œåä¹‹ï¼Œåˆ™ç”Ÿæˆçš„KDæ ‘æˆ–è€…çƒæ ‘ä¼šå°ï¼Œå±‚æ•°è¾ƒæµ…ï¼Œå»ºæ ‘æ—¶é—´è¾ƒçŸ­ã€‚é»˜è®¤æ˜¯30. è¿™ä¸ªå€¼ä¸€èˆ¬ä¾èµ–äºæ ·æœ¬çš„æ•°é‡ï¼Œéšç€æ ·æœ¬æ•°é‡çš„å¢åŠ ï¼Œè¿™ä¸ªå€¼å¿…é¡»è¦å¢åŠ ï¼Œå¦åˆ™ä¸å…‰å»ºæ ‘é¢„æµ‹çš„æ—¶é—´é•¿ï¼Œè¿˜å®¹æ˜“è¿‡æ‹Ÿåˆã€‚å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯æ¥é€‰æ‹©ä¸€ä¸ªé€‚ä¸­çš„å€¼ã€‚

* è·ç¦»åº¦é‡metric ï¼šKè¿‘é‚»æ³•å’Œé™å®šåŠå¾„æœ€è¿‘é‚»æ³•ç±»å¯ä»¥ä½¿ç”¨çš„è·ç¦»åº¦é‡è¾ƒå¤šï¼Œä¸€èˆ¬æ¥è¯´é»˜è®¤çš„æ¬§å¼è·ç¦»ï¼ˆå³p=2çš„é—µå¯å¤«æ–¯åŸºè·ç¦»ï¼‰å°±å¯ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ã€‚

ä»ä¸Šè¿°ç»“æœä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºåªæœ‰ROC-AUCæ¯”è¾ƒä½ã€‚æŒ‰ç†æ¥è¯´åº”è¯¥å…ˆå»åˆ†æä¸ºä»€ä¹ˆPR-AUCæ¯”è¾ƒé«˜ ï¼Œä½†æ˜¯ROC-AUCæ¯”è¾ƒä½ã€‚æ‰€ä»¥ä¸‹é¢å»å°è¯•æ–°çš„å‚æ•°ã€‚å°†n_neighborsè°ƒæ•´ä¸º5ã€‚


```python
neigh = KNeighborsClassifier(n_neighbors=5) # n_neighborsçš„ç¼ºçœå€¼æ˜¯5
neigh.fit(X_train, y_train) 

neigh.score(X_val, y_val)
pred = neigh.predict(X_val)
pred_prob = neigh.predict_proba(X_val)

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9410354745925216

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.9729519223960915

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9567275892080069

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.8864503802381525

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
area = auc(recall, precision)
# 0.9864611444916045
```

ä»ä¸Šé¢çš„ç»“æœæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼šROC-AUCæœ‰å¾ˆå¤§æå‡ï¼ŒåŒæ—¶å…¶ä½™çš„è¯„ä¼°æŒ‡æ ‡ä¹Ÿæœ‰æå‡ï¼Œæ€»ä½“æ¥è¯´æ•ˆæœå·²ç»å¾ˆå¥½ã€‚æˆ‘å°±è®¤ä¸ºè°ƒæ•´å‚æ•°çš„æ–¹å‘æ˜¯å¯¹çš„ã€‚ä¸‹é¢å†æ¬¡å°è¯•å°† n_neighbors è°ƒæ•´ä¸º7ï¼Œçœ‹çœ‹æ•ˆæœã€‚

```python
neigh = KNeighborsClassifier(n_neighbors=7) # n_neighborsçš„ç¼ºçœå€¼æ˜¯5
neigh.fit(X_train, y_train) 

neigh.score(X_val, y_val)
pred = neigh.predict(X_val)
pred_prob = neigh.predict_proba(X_val)

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9372578342736727

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.9762798272321744

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9563709509606714

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.9000309452263087

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
area = auc(recall, precision)
# 0.9880128918540668
```

ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹å‡ºï¼ŒROC-AUCä»æ—§æœ‰ä¸€äº›æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å°è¯•äº†å°†n_neighborsè°ƒæ•´ä¸º9ï¼ŒåŸºæœ¬ä¸Šå„ä¸ªè¯„ä¼°å‚æ•°å·²ç»ä¿æŒä¸å˜ã€‚

å¦å¤–ï¼Œå¯¹äºKNNæ¥è¯´è¿˜æœ‰å¦å¤–ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œå³è¿‘é‚»æƒé‡ weightsã€‚é»˜è®¤çš„"uniform"ï¼Œæ„å‘³ç€æ‰€æœ‰æœ€è¿‘é‚»æ ·æœ¬æƒé‡éƒ½ä¸€æ ·ï¼Œåœ¨åšé¢„æµ‹æ—¶ä¸€è§†åŒä»ã€‚å¦‚æœæ˜¯"distance"ï¼Œåˆ™æƒé‡å’Œè·ç¦»æˆåæ¯”ä¾‹ï¼Œå³è·ç¦»é¢„æµ‹ç›®æ ‡æ›´è¿‘çš„è¿‘é‚»å…·æœ‰æ›´é«˜çš„æƒé‡ï¼Œè¿™æ ·åœ¨é¢„æµ‹ç±»åˆ«æˆ–è€…åšå›å½’æ—¶ï¼Œæ›´è¿‘çš„è¿‘é‚»æ‰€å çš„å½±å“å› å­ä¼šæ›´åŠ å¤§ã€‚è¿™é‡Œæˆ‘å°è¯•å°†weightsè°ƒæ•´ä¸ºdistanceã€‚

```python
neigh = KNeighborsClassifier(n_neighbors=7, weights = 'distance') # n_neighborsçš„ç¼ºçœå€¼æ˜¯5
neigh.fit(X_train, y_train) 

neigh.score(X_val, y_val)
pred = neigh.predict(X_val)
pred_prob = neigh.predict_proba(X_val)

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9421898406645786

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.9797493450400057

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9606025894685688

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.9179012756198074

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
area = auc(recall, precision)
# 0.9901032113225808
```

ä»ä¸Šè¿°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œå‚æ•°åˆæœ‰å°å¹…åº¦çš„æå‡ã€‚


### æœ´ç´ è´å¶æ–¯

[æœ´ç´ è´å¶æ–¯æ–¹æ³•](https://www.cnblogs.com/pinard/p/6074222.html)æ˜¯ä¸€ç³»åˆ—æœ‰ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åŸºäºå¯¹è´å¶æ–¯ç†è®ºçš„åº”ç”¨ï¼Œå³ç®€å•(naive)çš„å‡è®¾ æ¯å¯¹ç‰¹å¾ä¹‹é—´éƒ½ç›¸äº’ç‹¬ç«‹ã€‚

é€šè¿‡[scikit-learnå®˜ç½‘APIè¯´æ˜](http://sklearn.lzjqsdd.com/modules/naive_bayes.html)å¯ä»¥çœ‹åˆ°ï¼Œsklearnå°†è´å¶æ–¯çš„ä¸‰ä¸ªå¸¸ç”¨æ¨¡å‹éƒ½å°è£…å¥½äº†ï¼Œåˆ†åˆ«æ˜¯ï¼šé«˜æ–¯è´å¶æ–¯ï¼ˆGaussian Naive Bayesï¼‰ã€å¤šé¡¹å¼è´å¶æ–¯ï¼ˆMultinomial Naive Bayesï¼‰ã€ä¼¯åŠªåˆ©è´å¶æ–¯ï¼ˆBernoulli Naive Bayesï¼‰ã€‚æ¥ç€å°±å¯ä»¥å­¦ä¹ å®ƒæ‰€ç»™å‡ºçš„ä¾‹ç¨‹äº†ã€‚

å„ç§å„æ ·çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œä»–ä»¬å¯¹ P(x_i|y) çš„åˆ†å¸ƒçš„è®¤è¯†å’Œå‡è®¾ä¸åŒã€‚

å°½ç®¡å®ƒä»¬çœ‹èµ·æ¥æœ‰ä¸€ä¸ªè¿‡äºç®€å•çš„å‡è®¾ï¼Œæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ä»ç„¶ åœ¨çœŸå®ä¸–ç•Œçš„è®¸å¤šæƒ…æ™¯ä¸‹å·¥ä½œè‰¯å¥½ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»å’Œåƒåœ¾é‚®ä»¶ç­›é€‰é¢†åŸŸå°¤å…¶æµè¡Œã€‚ å®ƒä»¬è¦æ±‚å°‘é‡çš„æ•°æ®æ¥ä¼°è®¡å¿…è¦çš„å‚æ•°ã€‚ (å…³äºç†è®ºä¸Šæœ´ç´ è´å¶æ–¯ä¸ºä»€ä¹ˆä¼šå·¥ä½œè‰¯å¥½ï¼Œä»¥åŠå®ƒå¯ä»¥é€‚ç”¨çš„æ•°æ®ç±»å‹ï¼Œè¯¦è§ä¸‹æ–¹References)

æœ´ç´ è´å¶æ–¯å­¦ä¹ å’Œåˆ†ç±»å™¨ä¸å…¶ä»–ç›¸æ¯”å¯ä»¥éå¸¸å¿«ã€‚ç±»æ¡ä»¶ç‰¹å¾åˆ†å¸ƒçš„è§£è€¦æ„å‘³ç€ æ¯ä¸ªåˆ†å¸ƒå¯ä»¥ç‹¬ç«‹ä¼°è®¡ä¸ºä¸€ä¸ªä¸€ç»´åˆ†å¸ƒï¼Œè¿™åè¿‡æ¥åˆæœ‰åŠ©äºç¼“è§£ç»´ç¾éš¾é—®é¢˜ã€‚

å¦ä¸€æ–¹é¢ï¼Œè™½ç„¶è¢«ç§°ä¸ºä¸€ä¸ªåˆé€‚çš„åˆ†ç±»å™¨ï¼Œå®ƒä¹Ÿè¢«è®¤ä¸ºæ˜¯æ˜¯ä¸€ä¸ªåçš„ä¼°è®¡é‡ï¼Œæ‰€ä»¥å¯¹ predict_proba çš„æ¦‚ç‡è¾“å‡ºä¸åº”å¤ªè¿‡ä¾èµ–ã€‚

#### æœ´ç´ è´å¶æ–¯ é«˜æ–¯æ¨¡å‹

ä¸‹é¢å°è¯•ä½¿ç”¨é«˜æ–¯è´å¶æ–¯å»ºæ¨¡ã€‚é«˜æ–¯è´å¶æ–¯ä¸€èˆ¬é€‚ç”¨äºæ ·æœ¬åˆ†å¸ƒç¬¦åˆæˆ–è€…ç±»ä¼¼é«˜æ–¯åˆ†å¸ƒçš„æ—¶å€™ä½¿ç”¨ã€‚

é«˜æ–¯æ¨¡å‹å‡è®¾è¿™äº›ä¸€ä¸ªç‰¹å¾çš„æ‰€æœ‰å±äºæŸä¸ªç±»åˆ«çš„è§‚æµ‹å€¼ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚GaussianNBç±»çš„ä¸»è¦å‚æ•°ä»…æœ‰ä¸€ä¸ªï¼Œå³å…ˆéªŒæ¦‚ç‡priors ï¼Œå¯¹åº”Yçš„å„ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ğ‘ƒ(ğ‘Œ=ğ¶ğ‘˜)ã€‚è¿™ä¸ªå€¼é»˜è®¤ä¸ç»™å‡ºï¼Œå¦‚æœä¸ç»™å‡ºæ­¤æ—¶ğ‘ƒ(ğ‘Œ=ğ¶ğ‘˜)=ğ‘šğ‘˜/ğ‘šã€‚å…¶ä¸­mä¸ºè®­ç»ƒé›†æ ·æœ¬æ€»æ•°é‡ï¼Œğ‘šğ‘˜ä¸ºè¾“å‡ºä¸ºç¬¬kç±»åˆ«çš„è®­ç»ƒé›†æ ·æœ¬æ•°ã€‚å¦‚æœç»™å‡ºçš„è¯å°±ä»¥priors ä¸ºå‡†ã€‚


```python

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train.toarray(), y_train)
pred = clf.predict(X_val.toarray())
pred_prob = clf.predict_proba(X_val.toarray())

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9906658369632856

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.789067478581038

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.8784486835882075

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.8962313785693254

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
auc(recall, precision)
# 0.98782882967254
```

æ­¤å¤–ï¼ŒGaussianNBä¸€ä¸ªé‡è¦çš„åŠŸèƒ½æ˜¯æœ‰ partial_fitæ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•çš„ä¸€èˆ¬ç”¨åœ¨å¦‚æœè®­ç»ƒé›†æ•°æ®é‡éå¸¸å¤§ï¼Œä¸€æ¬¡ä¸èƒ½å…¨éƒ¨è½½å…¥å†…å­˜çš„æ—¶å€™ã€‚è¿™æ—¶æˆ‘ä»¬å¯ä»¥æŠŠè®­ç»ƒé›†åˆ†æˆè‹¥å¹²ç­‰åˆ†ï¼Œé‡å¤è°ƒç”¨partial_fitæ¥ä¸€æ­¥æ­¥çš„å­¦ä¹ è®­ç»ƒé›†ï¼Œéå¸¸æ–¹ä¾¿ã€‚åé¢è®²åˆ°çš„MultinomialNBå’ŒBernoulliNBä¹Ÿæœ‰ç±»ä¼¼çš„åŠŸèƒ½ã€‚

#### æœ´ç´ è´å¶æ–¯ å¤šé¡¹å¼æ¨¡å‹

MultinomialNB å®ç°äº†æ•°æ®æœä»å¤šé¡¹å¼åˆ†å¸ƒæ—¶çš„è´å¶æ–¯ç®—æ³•ï¼Œå®ƒä¹Ÿæ˜¯æ–‡æœ¬åˆ†ç±»é¢†åŸŸçš„ ä¸¤ç§å…¸å‹ç®—æ³•ä¹‹ä¸€(è¿™é‡Œæ•°æ®é€šå¸¸ä»¥è¯å‘é‡çš„å½¢å¼è¡¨ç¤ºï¼Œtf-idfå‘é‡åœ¨è¿™é‡Œä¹Ÿè¡¨ç°çš„å¾ˆå¥½)ã€‚ è¿™ä¸ªåˆ†å¸ƒè¢«å‚æ•°åŒ–æˆå‘é‡ï¼š

```python
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(X_train.toarray(), y_train)
pred = clf.predict(X_val.toarray())
pred_prob = clf.predict_proba(X_val.toarray())

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9532094469534316

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.9116335056291156

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9319580166485704

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.9102314273603492

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
auc(recall, precision)
# 0.9896928463464106
```

#### ä¼¯åŠªåˆ©æ¨¡å‹

ç±» BernoulliNB å®ç°äº†å¯¹äºæœä»å¤šå…ƒä¼¯åŠªåˆ©åˆ†å¸ƒçš„æ•°æ®çš„æœ´ç´ è´å¶æ–¯è®­ç»ƒå’Œåˆ†ç±»ç®—æ³•ï¼› ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºå¤§é‡ç‰¹å¾ï¼Œæ¯ä¸€ä¸ªç‰¹å¾éƒ½æ˜¯ä¸€ä¸ª0-1å˜é‡ (Bernoulli, boolean)ã€‚ å› æ­¤ï¼Œè¿™ä¸ªç±»è¦æ±‚æ ·æœ¬é›†åˆä»¥0-1ç‰¹å¾å‘é‡çš„æ–¹å¼å±•ç°ã€‚å¦‚æœæ¥æ”¶åˆ°äº†å…¶ä»–ç±»å‹çš„æ•°æ®ä½œä¸ºå‚æ•°ï¼Œ ä¸€ä¸ª BernoulliNB å®ä¾‹ä¼šæŠŠè¾“å…¥æ•°æ®äºŒå…ƒåŒ–(å–å†³äº binarize å‚æ•°è®¾ç½®)

åœ¨æ–‡æœ¬åˆ†ç±»çš„æƒ…å¢ƒä¸­ï¼Œè¢«ç”¨æ¥è®­ç»ƒå’Œä½¿ç”¨è¿™ä¸€åˆ†ç±»å™¨çš„æ˜¯è¯è¯­åŒç°å‘é‡ (word occurrence vectors) è€Œä¸æ˜¯è¯é¢‘å‘é‡ (word count vectors)ã€‚ BernoulliNB å¯èƒ½å°¤å…¶ä¼šåœ¨å°æ•°æ®é›†æ—¶è¡¨ç°è‰¯å¥½ã€‚


```python
from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()

clf.fit(X_train.toarray(), y_train)
pred = clf.predict(X_val.toarray())
pred_prob = clf.predict_proba(X_val.toarray())

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9674234945705824

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.8326842738794874

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.8950112256935195

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.8858861253203398

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
auc(recall, precision)
# 0.9869821857107934
```

é€šè¿‡è¯•ç”¨ä»¥ä¸Šæ‰€æœ‰æœ´ç´ è´å¶æ–¯æ¨¡å‹ï¼Œæˆ‘å¯ä»¥çœ‹å‡ºçš„æ˜¯å¤šé¡¹å¼æ¨¡å‹åœ¨æ­¤å¥—æ–°é—»è¯­æ–™æ–‡æœ¬åˆ†ç±»ä¸Šæ•ˆæœæ›´å¥½ã€‚


### é€»è¾‘å›å½’

```python

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')

clf.fit(X_train, y_train) 

pred = clf.predict(X_val)
pred_prob = clf.predict_proba(X_val)

# precision = TP/(TP + FP)
precision_score(y_val, pred) 
# 0.9776720095191432

# recall = TP/(TP + FN)
recall_score(y_val, pred)
# 0.9890249946895135

# 1/F1 = 1/2 * (1/P + 1/R) or 1/F1 = 2PR/(P + R)
f1_score(y_val, pred)
# 0.9833157338965153

# ROC-AUC
roc_auc_score(y_val, pred_prob[:, 1])
# 0.9891842749550868

# PR-AUC
precision, recall, thresholds = precision_recall_curve(y_val, pred_prob[:,1])
area = auc(recall, precision)
# 0.9987933183957046
```

ä½¿ç”¨é€»è¾‘å›å½’å¯¹æ­¤æ–‡æœ¬åˆ†ç±»çš„æ•ˆæœæœ€ä½³ã€‚

### å¯»æ‰¾æŠ„è¢­æ–°åç¤¾çš„æ–‡ç« 

æˆ‘ä»¬å¯ä»¥é¦–å…ˆä½¿ç”¨é€»è¾‘å›å½’çš„æ¨¡å‹å»åˆ¤æ–­ä¸€ç¯‡æ–‡ç« æ˜¯å¦æ˜¯æ–°åç¤¾çš„æ–‡ç« ï¼Œå¦‚æœåˆ¤æ–­å‡ºæ¥æ˜¯æ–°åç¤¾çš„ï¼Œä½†æ˜¯ï¼Œå®ƒçš„sourceå¹¶ä¸æ˜¯æ–°åç¤¾çš„ï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬å°±è¯´ï¼Œè¿™ä¸ªæ–‡ç« æ˜¯æŠ„è¢­çš„æ–°åç¤¾çš„æ–‡ç« ã€‚å³predictçš„labelä¸º1ï¼Œä½†æ˜¯å®é™…ä¸Šlabelä¸º0çš„æ–°é—»ã€‚

```python

pred_all = clf.predict(X)

plagiarized = []
for i in range(len(y)):
	if pred_all[i] == 1 and y[i] == 0:
		plagiarized.append(i)

news_nona.index = range(len(y))
plagiarized_news = news_nona.ix[plagiarized, :]
len(plagiarized) # 1643
```

å› ä¸ºæ ·æœ¬çš„æ­£ç±»å’Œè´Ÿç±»æ¯”ä¾‹å¤§çº¦ä¸º10:1ï¼Œæ‰€ä»¥å€¾å‘äºæœ‰æ›´å¤šçš„è´Ÿç±»ä¼šå½’ä¸ºæ­£ç±»ï¼Œä¹Ÿå°±æ˜¯è¯¯åˆ¤ç‡ï¼ˆå‡é˜³æ€§ç‡ï¼‰ä¼šæ¯”è¾ƒé«˜ã€‚æ‰€ä»¥åº”é€‚å½“æå‡é€»è¾‘å›å½’ä¸­è®¾å®šçš„æ­£ç±»é˜ˆå€¼ã€‚

