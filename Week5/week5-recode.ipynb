{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何通过词向量获取相近意思的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "读取文件并替换掉NA\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "csv_path = '/data/tusers/lixiangr/caill/NLP/data/sqlResult_1558435.csv'\n",
    "content = pd.read_csv(csv_path, encoding = 'gb18030')\n",
    "content = content.fillna(\"\")\n",
    "content.head()\n",
    "content.columns.tolist()\n",
    "news_content = content[\"content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义正则表达式匹配（为了去除特殊符号）以及结巴分词的函数\n",
    "/\\w/g匹配所有的阿拉伯数字、大小写字母、下划线；\n",
    "/\\d/g匹配数字\n",
    "\"\"\"\n",
    "\n",
    "import jieba\n",
    "def cut(string): return \" \".join(jieba.cut(string))\n",
    "cut(\"这是一个测试例子，结果怎么样\")\n",
    "\n",
    "import re\n",
    "def token(string): return re.findall(\"[\\d|\\w]+\", string)\n",
    "token(\"token\\('这是一个测试\\n\\n\\n'\\)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "去除特殊符号，如\"(\", \")\", \",\", \"\\r\", \"\\n\"\n",
    "将结巴分词得到的list进行连接得到纯文本\n",
    "\"\"\"\n",
    "\n",
    "news_content = [token(a) for a in news_content]\n",
    "news_content = [' '.join(a) for a in news_content]\n",
    "news_content = [cut(a) for a in news_content]\n",
    "\n",
    "with open(\"/data/tusers/lixiangr/caill/NLP/data/news-sentences-cut.txt\", \"w\") as f:\n",
    "    for n in news_content:\n",
    "        f.write(n + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Word2Vec的参数：\n",
    "size: 是指输出的词的向量维数，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。\n",
    "workers: 参数控制训练的并行数。\n",
    "\"\"\"\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "news_word2vec = Word2Vec(LineSentence(\"/data/tusers/lixiangr/caill/NLP/data/news-sentences-cut.txt\"), size = 35, workers = 8)\n",
    "len(news_word2vec.wv.vocab)\n",
    "\n",
    "news_word2vec.most_similar(\"葡萄牙\", topn = 20)\n",
    "news_word2vec.most_similar(\"捷克\", topn = 20)\n",
    "news_word2vec.most_similar(\"说\", topn = 20)\n",
    "news_word2vec.most_similar(\"怒斥\", topn = 20)\n",
    "news_word2vec.most_similar(\"认为\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据越多，效果就会越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 可以使用动态规划优化计算速度\n",
    "def get_related_words(initial_words, model):\n",
    "    \n",
    "    unseen = initial_words\n",
    "    seen = defaultdict(int)\n",
    "    \n",
    "    # 可以更大，控制寻找的相近词的数量\n",
    "    max_size = 500\n",
    "    \n",
    "    while unseen and len(seen) < max_size:\n",
    "        if len(seen) % 50 == 0: print('seen length : {}'.format(len(seen)))\n",
    "            \n",
    "        node = initial_words.pop(0)\n",
    "        new_expanding = [s for s, t in news_word2vec.most_similar(node, topn = 20)]\n",
    "        \n",
    "        unseen += new_expanding\n",
    "        seen[node] += 1 # 这个1可以修改\n",
    "    \n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_words = get_related_words([\"说\", \"表示\"], news_word2vec)\n",
    "sorted(related_words.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 某一个单词在多少个文本中出现，出现这个单词的文本越多那个tf-idf越大\n",
    "def document_frequency(word): \n",
    "    return sum(1 for n in news_content if word in n)\n",
    "\n",
    "# 总的文本数与出现这个单词的文本数的比值\n",
    "def idf(word):\n",
    "    \"\"\"Gets the inversed document frequency\"\"\"\n",
    "    return math.log10(len(news_content) / document_frequency(word))\n",
    "\n",
    "# 某个单词在某个文本中的出现次数\n",
    "def tf(word, document):\n",
    "    \"\"\"\n",
    "    Gets the term frequemcy of a @word in a @document.\n",
    "    \"\"\"\n",
    "    words = document.split()\n",
    "    \n",
    "    return sum(1 for w in words if w == word)\n",
    "\n",
    "\"\"\"\n",
    "某个单词在目标文本中的出现次数（越多越重要）*总的文本数与出现这个单词的文本数的比值（越大就代表出现某个单词的文本数少）\n",
    "返回的是目标文本中每一个单词的tf-idf值\n",
    "\"\"\"\n",
    "def get_keywords_of_a_ducment(document):\n",
    "    words = set(document.split())\n",
    "    \n",
    "    tfidf = [\n",
    "        (w, tf(w, document) * idf(w)) for w in words\n",
    "    ]\n",
    "    \n",
    "    tfidf = sorted(tfidf, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "\n",
    "document_frequency('的')\n",
    "news_content[0]\n",
    "idf('的') < idf('小米')\n",
    "content['content'][11]\n",
    "tf('银行', news_content[11])\n",
    "tf('创业板', news_content[11])\n",
    "idf('创业板')\n",
    "idf('银行')\n",
    "idf('短期')\n",
    "tf('短期', news_content[11])\n",
    "\n",
    "news_content[0]\n",
    "news_content[11]\n",
    "%prun get_keywords_of_a_ducment(news_content[0])\n",
    "machine_new_keywords = get_keywords_of_a_ducment(news_content[101])\n",
    "news_content[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云(WordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# we could download the font from https://github.com/Computing-Intelligence/datasource\n",
    "wc = wordcloud.WordCloud('/data/tusers/lixiangr/caill/NLP/data/datasource/SourceHanSerifSC-Regular.otf')\n",
    "\n",
    "news_content[4]\n",
    "help(wc.generate_from_frequencies)\n",
    "machine_new_keywords_dict = {w: score for w, score in machine_new_keywords}\n",
    "plt.imshow(wc.generate_from_frequencies(machine_new_keywords_dict))\n",
    "shenzhen_social_news = get_keywords_of_a_ducment(news_content[4])\n",
    "shenzhen_social_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "police_mask = np.array(Image.open('/data/tusers/lixiangr/caill/NLP/week5_recode/wordvec.png'))\n",
    "wordcloud_with_mask = wordcloud.WordCloud(font_path='/data/tusers/lixiangr/caill/NLP/data/datasource/SourceHanSerifSC-Regular.otf', mask=police_mask)\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "# 中文和负号的正常显示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.imshow(wc.generate_from_frequencies({w:s for w, s in shenzhen_social_news[:20]}))\n",
    "plt.savefig(\"/data/tusers/lixiangr/caill/NLP/week5_recode/wordcloud_1.pdf\")\n",
    "plt.close()\n",
    "    \n",
    "plt.switch_backend('agg')\n",
    "# 中文和负号的正常显示\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.imshow(wordcloud_with_mask.generate_from_frequencies({w:s for w, s in shenzhen_social_news[:20]}))\n",
    "plt.savefig(\"/data/tusers/lixiangr/caill/NLP/week5_recode/wordcloud_2.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "vectorized = TfidfVectorizer(max_features = 10000)\n",
    "vectorized.vocabulary_\n",
    "\n",
    "sample_num = 5000\n",
    "sub_samples = news_content[:sample_num]\n",
    "X = vectorized.fit_transform(sub_samples)\n",
    "X.shape\n",
    "\n",
    "# 获得值不为0的位置\n",
    "np.where(X[0].toarray())\n",
    "\n",
    "document_id_1, document_id_2 = random.randint(0, 1000), random.randint(0, 1000)\n",
    "news_content[document_id_1]\n",
    "news_content[document_id_2]\n",
    "\n",
    "vector_of_d_1 = X[document_id_1].toarray()[0]\n",
    "vector_of_d_2 = X[document_id_2].toarray()[0]\n",
    "\n",
    "random_choose = random.randint(0, 1000)\n",
    "news_content[random_choose]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance  import cosine\n",
    "\n",
    "def distance(v1, v2): return cosine(v1, v2)\n",
    "\n",
    "distance([1, 1], [2, 2])\n",
    "\n",
    "distance(X[random_choose].toarray()[0], X[document_id_1].toarray()[0])\n",
    "distance(X[random_choose].toarray()[0], X[document_id_2].toarray()[0])\n",
    "\n",
    "sorted(list(range(5000)), key = lambda i: distance(X[random_choose].toarray()[0], X[i].toarray()[0]))\n",
    "\n",
    "# bin() 返回一个整数 int 或者长整数 long int 的二进制表示。\n",
    "bin(49 & 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立搜索引擎\n",
    "- Input: Words\n",
    "- Output: Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_search(keywords):\n",
    "    news_ids = [i for i, n in enumerate(news_content) if all(w in n for w in keywords)]\n",
    "    return news_ids\n",
    "    # O(D * w) \n",
    "\n",
    "news_ids = naive_search('美军 司令 航母'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input word -> the documents which contain this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "transposed_x = X.transpose().toarray()\n",
    "word_2_id = vectorized.vocabulary_\n",
    "word_2_id['今天']\n",
    "id_2_word = {d: w for w, d in word_2_id.items()}\n",
    "set(np.where(transposed_x[6195])[0])\n",
    "word_2_id['美军']\n",
    "word_2_id['司令']\n",
    "usa_force = set(np.where(transposed_x[7922])[0])\n",
    "commander = set(np.where(transposed_x[2769])[0])\n",
    "usa_force & commander\n",
    "from functools import reduce\n",
    "d1, d2, d3 = {1, 2, 3}, {4, 5, 6, 3, 2}, {1, 3, 4}\n",
    "from operator import and_\n",
    "reduce(and_, [d1, d2, d3])\n",
    "\n",
    "def search_engine(query):\n",
    "    \"\"\"\n",
    "    @query is the searched words, splited by space\n",
    "    @return is the related documents which ranked by tfidf similarity\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "    query_vec = vectorized.transform([' '.join(words)]).toarray()[0]\n",
    "    candidates_ids = [word_2_id[w] for w in words]\n",
    "    documents_ids = [\n",
    "         set(np.where(transposed_x[_id])[0]) for _id in candidates_ids\n",
    "    ]\n",
    "    merged_documents = reduce(and_, documents_ids)\n",
    "    # we could know the documents which contain these words\n",
    "    sorted_docuemtns_id = sorted(merged_documents, key=lambda i: distance(query_vec, X[i].toarray()))\n",
    "    return sorted_docuemtns_id\n",
    "\n",
    "np.where(vectorized.transform(['美联储 加息 次数']).toarray()[0])\n",
    "text = \"\"\"新华社洛杉矶４月８日电（记者黄恒）美国第三舰队８日发布声明说，该舰队下属的“卡尔·文森”航母战斗群当天离开新加坡，改变原定驶往澳大利亚的任务计划，转而北上，前往西太平洋朝鲜半岛附近水域展开行动。\\n　　该舰队网站主页发布的消息说，美军太平洋司令部司令哈里·哈里斯指示“卡尔·文森”航母战斗群向北航行。这一战斗群包括“卡尔·文森”号航空母舰、海军第二航空队、两艘“阿利·伯克”级导弹驱逐舰和一艘“泰孔德罗加”级导弹巡洋舰。\\n　　“卡尔·文森”号航母的母港位于美国加利福尼亚州的圣迭戈，今年１月初前往西太平洋地区执行任务，并参与了日本及韩国的军事演习。\\n　　美国有线电视新闻网援引美国军方官员的话说，“‘卡尔·文森’号此次行动是为了对近期朝鲜的挑衅行为作出回应”。（完）\"\"\"\n",
    "print(text)\n",
    "\n",
    "import re\n",
    "text = \"\"\"美国有线电视新闻网援引美国军方官员的话说\"\"\"\n",
    "pat = r'(新闻|官员)'\n",
    "re.compile(pat).sub(repl=\"**\\g<1>**\", string=text)\n",
    "def get_query_pat(query):\n",
    "    return re.compile('({})'.format('|'.join(query.split())))\n",
    "\n",
    "get_query_pat('美军 司令 航母')\n",
    "\n",
    "def highlight_keywords(pat, document):\n",
    "    return pat.sub(repl=\"**\\g<1>**\", string=document) \n",
    "\n",
    "highlight_keywords(get_query_pat('美军 司令 航母'), content['content'][22987])\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def search_engine_with_pretty_print(query):\n",
    "    candidates_ids = search_engine(query)\n",
    "    for i, _id in enumerate(candidates_ids):\n",
    "        title = '## Search Result {}'.format(i)\n",
    "        c = content['content'][_id]\n",
    "        c = highlight_keywords(get_query_pat(query), c)    \n",
    "        \n",
    "        display(Markdown(title + '\\n' + c))\n",
    "        \n",
    "search_engine_with_pretty_print('春节 假期')\n",
    "search_engine()\n",
    "#%%timeit\n",
    "search_engine('美联储 加息 次数')\n",
    "\n",
    "content['content'][2189]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from string import ascii_uppercase\n",
    "ascii_uppercase\n",
    "\n",
    "def genearte_random_website():\n",
    "    return ''.join([random.choice(ascii_uppercase) for _ in range(random.randint(3, 5))]) + '.'  + random.choice(['com', 'cn', 'net'])\n",
    "\n",
    "genearte_random_website()\n",
    "websites = [genearte_random_website() for _ in range(25)]\n",
    "\n",
    "websites\n",
    "\n",
    "random.sample(websites, 10)\n",
    "\n",
    "website_connection = {\n",
    "    websites[0]: random.sample(websites, 10),\n",
    "    websites[1]: random.sample(websites, 5),\n",
    "    websites[3]: random.sample(websites, 7),\n",
    "    websites[4]: random.sample(websites, 2),\n",
    "    websites[5]: random.sample(websites, 1),\n",
    "}\n",
    "\n",
    "website_network = nx.graph.Graph(website_connection)\n",
    "plt.figure(3,figsize=(12,12))\n",
    "nx.draw_networkx(website_network, font_size=10)\n",
    "\n",
    "sorted(nx.pagerank(website_network).items(),key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
